Kaynaklar
- [Synthetic DATA Generation using LANGCHAIN ğŸ¦œï¸ğŸ”—](# Synthetic DATA Generation using LANGCHAIN ğŸ¦œï¸ğŸ”—)
	- Related ipynb [notebook](https://github.com/sudarshan-koirala/youtube-stuffs/blob/main/langchain/synthetic_data_generation.ipynb)
- [How to use LLAMA3.1 with LANGCHAIN for free](https://www.youtube.com/watch?v=jP2llR7ZtwM)
- Langchain Synthetic data generation example [scripts](https://python.langchain.com/v0.1/docs/use_cases/data_generation/)
- KullanÄ±lan dataset: https://huggingface.co/datasets/musabg/wikipedia-tr


Yorumlar
- Datasetinin her bir entrysindeki textleri daha kÃ¼Ã§Ã¼k chunklara ayÄ±rdÄ±m. ChunklarÄ±n her biri en az 320 kelimeden oluÅŸacak ÅŸekilde paragraflarla split edildi.
- Her bir entry iÃ§in random olarak bir tane chunk'Ä± context olarak seÃ§ip yeni oluÅŸturduÄŸum "context" column'unda kaydettim.
- **Langchain** kullanarak **llama-3.1-70b-versatile** modelini **groq** yardÄ±mÄ±yla bu entrylerin ilk 4970 tanesi iÃ§in question Ã¼rettirdim. 
- Elimdeki verileri Multilingual bir sentence transformers modeli olan AlibabaNLP ile test ettim.
- 4970 entry ve tÃ¼m entrylerin ilk 50k sÄ± vector database olarak kullanÄ±larak alÄ±nan sonuÃ§ ÅŸu ÅŸekilde:
	- Relevant answer in first answer:  3358 / 4970
	- Relevant answers in top-5:  3995 / 4970
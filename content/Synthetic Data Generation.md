Kaynaklar
- [Synthetic DATA Generation using LANGCHAIN ğŸ¦œï¸ğŸ”—](# Synthetic DATA Generation using LANGCHAIN ğŸ¦œï¸ğŸ”—)
	- Related ipynb [notebook](https://github.com/sudarshan-koirala/youtube-stuffs/blob/main/langchain/synthetic_data_generation.ipynb)
- [How to use LLAMA3.1 with LANGCHAIN for free](https://www.youtube.com/watch?v=jP2llR7ZtwM)
- Langchain Synthetic data generation example [scripts](https://python.langchain.com/v0.1/docs/use_cases/data_generation/)
- KullanÄ±lan dataset: https://huggingface.co/datasets/musabg/wikipedia-tr


Yorumlar
- Datasetinin her bir entrysindeki textleri daha kÃ¼Ã§Ã¼k chunklara ayÄ±rdÄ±m. ChunklarÄ±n her biri en az 320 kelimeden oluÅŸacak ÅŸekilde paragraflarla split edildi.
- Her bir entry iÃ§in random olarak bir tane chunk'Ä± context olarak seÃ§ip yeni oluÅŸturduÄŸum "context" column'unda kaydettim.
- **Langchain** kullanarak **llama-3.1-70b-versatile** modelini **groq** yardÄ±mÄ±yla bu entrylerin ilk 4970 tanesi iÃ§in question Ã¼rettirdim. 
- Elimdeki verileri Multilingual bir sentence transformers modeli olan AlibabaNLP ile test ettim.
- 4970 entry ve tÃ¼m entrylerin ilk 50k sÄ± vector database olarak kullanÄ±larak alÄ±nan sonuÃ§ ÅŸu ÅŸekilde:
	- Relevant answer in first answer:  3358 / 4970
	- Relevant answers in top-5:  3995 / 4970
- YukarÄ±daki veriler doÄŸrultusunda model yetersiz bulunursa daha Ã§ok sentetik question Ã¼rettirilerek finetune alÄ±nabilir.


todo:
- [x] Inference sonuÃ§larÄ± metriklerine bak (wikirag-tr, outpus.json, sentetik veri)
	- [x] map@1 
	- [x] map@5 
	- [ ] ndcg@5 -> ndcg@k hesaplamak iÃ§in yeterince **gerÃ§ek** deÄŸer yok. Åuanki datasetlerinde her birinin **sadece 1 tane gerÃ§ek context eÅŸleÅŸmesi** var. Yani @k iÃ§inde bir relevancy sÄ±rasÄ± yapamÄ±yoruz
	- [ ] ndcg@10 
- [ ] finetune alÄ±p performans iyileÅŸmesi oluup olmadÄ±ÄŸÄ±na bak
- [ ] iyileÅŸme durumuna hem wikirag-tr hem output.json a bak


## Alibaba-NLP/gte-multilingual-base inference sonuÃ§larÄ±

| **datasets / metrics** | **map@1** | **map@5** | **ndcg@5** | **ndcg@10** |
| :--------------------: | :-------: | :-------: | :--------: | :---------: |
|    **output.json**     |   0.41    |   0.59    |     x      |      x      |
|     **wikirag-tr**     |   0.66    |   0.82    |     x      |      x      |
|   **sentetik_4970**    |   0.68    |   0.73    |     x      |      x      |

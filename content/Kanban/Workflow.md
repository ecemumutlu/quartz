---

kanban-plugin: board

---

## Inactive

- [ ] deepmind rl dersi #low
- [ ] Intro to DL book #ask
- [ ] [Building with Instruction-Tuned LLMs: A Step-by-Step Guide](https://www.youtube.com/watch?v=eTieetk2dSw)
- [ ] [[5-lms_are_unsupervised_multitask_learners.pdf]]


## To do

- [ ] embedding modellerine ve bu modeller nasıl ve ne gibi bir veriset ile train/finetune oluyor konularına da bakabilirsin #meetingNotes
- [ ] Tensorflow Neural Machine Translation (seq2seq) Tutorial #ask
- [ ] self-attention yapısına bakabilirsin multihead #meetingNotes
- [ ] [Topic 4: What is JEPA?](https://www.turingpost.com/p/jepa)
- [ ] GTE (generalized text embeddings)
- [ ] sentence bert [blogpost](https://towardsdatascience.com/sbert-deb3d4aef8a4)
- [ ] advanced python kursu
- [ ] codebase i conda environment ına çevir
- [ ] kodları snorlax formatına çevir
- [ ] baştan sona sentence transformer train et


## In Progress

- [ ] Skim through [[8-LayerNormalizationInTheTransformer.pdf|layer norm]]
- [ ] virtual env
- [ ] Tensor kütüphanesi dokümentasyonu oku
- [ ] RAG işleme yapılarına bak #meetingNotes
- [ ] Llama-3


## Done

- [ ] Jalammar [attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- [ ] RNN Intro [Video](https://www.youtube.com/watch?v=UNmqTiOnRfg)
- [ ] [Jalammar transformers](https://jalammar.github.io/illustrated-transformer/)
- [ ] Attention is all you need
- [ ] Coursera find a course #ask
- [ ] Coursera module-1 week-2
- [ ] [[2-llama1.pdf|Llama-1]]
- [ ] Coursera DLS module1 week3
- [ ] Byte-pair encoding: https://www.geeksforgeeks.org/byte-pair-encoding-bpe-in-nlp/
- [ ] Llama1 summary
- [ ] [Llama2 related GQA](https://medium.com/@raisomya360/demystifying-sliding-window-grouped-query-attention-a-simpler-approach-to-efficient-neural-6fb03b7d021f)
- [ ] module1 week3 [assignment](https://github.com/abdur75648/Deep-Learning-Specialization-Coursera/blob/main/Neural%20Networks%20and%20Deep%20Learning/Week3/Planar%20data%20classification%20with%20one%20hidden%20layer/Planar_data_classification_with_onehidden_layer_v6c.ipynb) inceleme
- [ ] Coursera DLS module1 week4
- [ ] IBM LLama2 [blogpost](https://www.ibm.com/topics/llama-2)
- [ ] Coursera DLS module1 week4 assignment
- [ ] Coursera DLS module2 week1
- [ ] [[3-llama2.pdf|Llama-2]]
- [ ] Llama2 summary
- [ ] [Sentence Transformers](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/#sentence-transformers) #meetingNotes
- [ ] ssh configs
- [ ] micro editor
- [ ] Coursera DLS module2 week2
- [ ] [Training and Finetuning Embedding Models with Sentence Transformers v3](https://huggingface.co/blog/train-sentence-transformers) #meetingNotes
- [ ] [Building LLM Applications: Introduction (Part 1)](https://medium.com/@vipra_singh/building-llm-applications-introduction-part-1-1c90294b155b#4d28) #meetingNotes
- [ ] RAG [videosu](https://www.youtube.com/watch?v=tcqEUSNCn8I) izlendi
- [ ] coursera dls module2 week3
- [ ] ml lifecycle problemi üzerine düşün
- [ ] dls module2 week1 assignment
- [ ] ["bi-encoder" ve "cross-encoder" ](https://www.sbert.net/examples/applications/cross-encoder/README.html)kavramları.
- [ ] ML Döngüsü Nedir ve Ne Yapıyoruz? #educationalMeeting
- [ ] sentence transformer eğitmek için kullanılmış bir dataset bul #meetingNotes şu formatta kulllanıldı, böyle böyle bir şeymiş vs, türkçe dataset bulamayabilirsin şimdilik ingilizce de bakabilrisin
- [ ] rag blogpost part 5


## To be planned

- [ ] sentence transformer environmentında model train etme/inference
- [ ] baştan sona türkçe sentence transformers model eğitimi
- [ ] llama3
- [ ] llama.cpp ve llama.cpp-python kütüphaneleri incelensin
- [ ] llama.cpp ve gguf formatlarını incele
- [ ] quantization araştırması - lmstudio




%% kanban:settings
```
{"kanban-plugin":"board","list-collapse":[false,false,false,false,false]}
```
%%
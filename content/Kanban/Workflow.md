---

kanban-plugin: board

---

## Inactive

- [ ] deepmind rl dersi #low


## To Do

- [ ] Llama-3
- [ ] [[5-lms_are_unsupervised_multitask_learners.pdf]]
- [ ] RAG işleme yapılarına bak #meetingNotes
- [ ] embedding modellerine ve bu modeller nasıl ve ne gibi bir veriset ile train/finetune oluyor konularına da bakabilirsin #meetingNotes
- [ ] Tensorflow Neural Machine Translation (seq2seq) Tutorial #ask
- [ ] self-attention yapısına bakabilirsin multihead #meetingNotes
- [ ] [Topic 4: What is JEPA?](https://www.turingpost.com/p/jepa)
- [ ] ML Döngüsü Nedir ve Ne Yapıyoruz? #educationalMeeting
- [ ] Intro to DL book #ask
- [ ] sentence bert
- [ ] GTE (generalized text embeddings)
- [ ] [Sentence Transformers](https://osanseviero.github.io/hackerllama/blog/posts/sentence_embeddings/#sentence-transformers)
- [ ] [Training and Finetuning Embedding Models with Sentence Transformers v3](https://huggingface.co/blog/train-sentence-transformers)
- [ ] [Building LLM Applications: Introduction (Part 1)](https://medium.com/@vipra_singh/building-llm-applications-introduction-part-1-1c90294b155b#4d28)


## In Progress

- [ ] [[3-llama2.pdf|Llama-2]]


## Done

- [ ] Jalammar [attention](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)
- [ ] RNN Intro [Video](https://www.youtube.com/watch?v=UNmqTiOnRfg)
- [ ] [Jalammar transformers](https://jalammar.github.io/illustrated-transformer/)
- [ ] Attention is all you need
- [ ] Coursera find a course #ask
- [ ] Coursera module-1 week-2
- [ ] [[2-llama1.pdf|Llama-1]]




%% kanban:settings
```
{"kanban-plugin":"board","list-collapse":[false,false,false,false]}
```
%%